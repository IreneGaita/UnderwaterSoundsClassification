{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Setup Librerie",
   "id": "4264284b9b949f7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T08:59:02.905706Z",
     "start_time": "2024-06-08T08:58:41.128506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import os\n",
    "import csv\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "from Dataset import CustomDataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "fa36b13e37f42fc9",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Definizione del dataset e dei data loader",
   "id": "957c6abc2e76d6c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T23:55:29.224097Z",
     "start_time": "2024-06-07T23:55:29.210410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted([d for d in os.listdir(self.root_dir) if os.path.isdir(os.path.join(self.root_dir, d))])\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "\n",
    "        for idx, cls in enumerate(self.classes):\n",
    "            class_dir = os.path.join(self.root_dir, cls)\n",
    "            for file in os.listdir(class_dir):\n",
    "                file_path = os.path.join(class_dir, file)\n",
    "                if os.path.isfile(file_path):\n",
    "                    self.data.append(file_path)\n",
    "                    self.targets.append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target"
   ],
   "id": "cd0ea1a4be5f2e82",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Checkpoint",
   "id": "5fdb94f4cbff5582"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T23:55:29.228863Z",
     "start_time": "2024-06-07T23:55:29.224097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    start_epoch += 1\n",
    "    metrics = checkpoint['metrics']\n",
    "    return model, optimizer, start_epoch, metrics\n",
    "\n",
    "\n",
    "def save_checkpoint(checkpoint_state, checkpoint_path):\n",
    "    torch.save(checkpoint_state, checkpoint_path)"
   ],
   "id": "fbd52149443558ca",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Calcolo Metriche",
   "id": "9ebe9fe192ff2235"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T23:55:29.234738Z",
     "start_time": "2024-06-07T23:55:29.228863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_metrics(loader, model, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    pbar = tqdm(loader, desc=\"Calcolo metriche\")\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.round())\n",
    "            pbar.update()\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ],
   "id": "c85186a9c65481d1",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preparazione Dati per il Modello",
   "id": "97d6490f6148564f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T23:55:46.453402Z",
     "start_time": "2024-06-07T23:55:29.241749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Dataset import CustomDataset\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    train_dataset_path = r'C:\\Users\\biagi\\PycharmProjects\\gruppo17\\Bilanciamento_Allenamento'\n",
    "    val_dataset_path = r'C:\\Users\\biagi\\PycharmProjects\\gruppo17\\Validazione_Norm'\n",
    "\n",
    "    train_dataset = CustomDataset(train_dataset_path, transform=transform)\n",
    "    val_dataset = CustomDataset(val_dataset_path, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)"
   ],
   "id": "cb3a9cca96137897",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#   Definizione del modello",
   "id": "1e3ece48501fd90"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T23:55:46.876023Z",
     "start_time": "2024-06-07T23:55:46.453402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    # Caricamento del modello EfficientNet B3 pre-addestrato\n",
    "    model = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.DEFAULT)\n",
    "\n",
    "    # Modifica dell'ultimo strato per la classificazione binaria\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, 1)\n",
    "\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    scaler = GradScaler()"
   ],
   "id": "a3ecf8326d221271",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Set Parametri e Cartella Salvataggio",
   "id": "d94704472c743903"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T23:55:47.110810Z",
     "start_time": "2024-06-07T23:55:46.876023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    checkpoint_dir = 'EfficientNetB3_Binary'\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    latest_checkpoint = glob.glob(os.path.join(checkpoint_dir, 'best_checkpoint.pth'))\n",
    "    if latest_checkpoint:\n",
    "        latest_checkpoint = max(latest_checkpoint, key=os.path.getctime)\n",
    "        model, optimizer, start_epoch, metrics = load_checkpoint(latest_checkpoint, model, optimizer)\n",
    "    else:\n",
    "        model, optimizer, start_epoch, metrics = model, optimizer, 0, []\n",
    "\n",
    "    num_epochs = 50\n",
    "    checkpoint_interval = 1\n",
    "    patience = 5\n",
    "    performance_drop_patience = 3\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    performance_drop_counter = 0\n",
    "    last_val_metrics = None\n",
    "    gradient_accumulation_steps = 4"
   ],
   "id": "a2526ad8bd219a64",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Allenamento",
   "id": "a5a8c9bed4b0de7f"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-07T23:55:47.110810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        pbar_train = tqdm(train_loader, desc=f\"Epoca {epoch + 1}/{num_epochs} - Addestramento\")\n",
    "        for step, (inputs, labels) in enumerate(pbar_train):\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                train_loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "            scaler.scale(train_loss).backward()\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            running_train_loss += train_loss.item()\n",
    "            pbar_train.set_postfix({'Loss': running_train_loss / (step + 1)})\n",
    "\n",
    "        validation_interval = 1\n",
    "\n",
    "        if (epoch + 1) % validation_interval == 0:\n",
    "            model.eval()\n",
    "            running_val_loss = 0.0\n",
    "            pbar_val = tqdm(val_loader, desc=f\"Epoca {epoch + 1}/{num_epochs} - Valutazione\")\n",
    "            for inputs, labels in pbar_val:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                with torch.no_grad():\n",
    "                    with autocast():\n",
    "                        outputs = model(inputs)\n",
    "                        val_loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "                    running_val_loss += val_loss.item()\n",
    "                    pbar_val.set_postfix({'Val Loss': running_val_loss / len(val_loader)})\n",
    "\n",
    "            train_accuracy, train_precision, train_recall, train_f1 = calculate_metrics(train_loader, model, device)\n",
    "            val_accuracy, val_precision, val_recall, val_f1 = calculate_metrics(val_loader, model, device)\n",
    "            epoch_train_loss = running_train_loss / len(train_loader)\n",
    "            epoch_val_loss = running_val_loss / len(val_loader)\n",
    "\n",
    "            print(f\"Epoca {epoch + 1}/{num_epochs} - \"\n",
    "                  f\"Loss Addestramento: {epoch_train_loss:.4f} - Acc Addestramento: {train_accuracy:.4f} - \"\n",
    "                  f\"Precision Addestramento: {train_precision:.4f} - Recall Addestramento: {train_recall:.4f} - \"\n",
    "                  f\"F1 Addestramento: {train_f1:.4f} - \"\n",
    "                  f\"Loss Valutazione: {epoch_val_loss:.4f} - Acc Valutazione: {val_accuracy:.4f} - \"\n",
    "                  f\"Precision Valutazione: {val_precision:.4f} - Recall Valutazione: {val_recall:.4f} - F1 Valutazione: {val_f1:.4f}\")\n",
    "\n",
    "            metrics.append({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": epoch_train_loss,\n",
    "                \"train_accuracy\": train_accuracy,\n",
    "                \"train_precision\": train_precision,\n",
    "                \"train_recall\": train_recall,\n",
    "                \"train_f1\": train_f1,\n",
    "                \"val_loss\": epoch_val_loss,\n",
    "                \"val_accuracy\": val_accuracy,\n",
    "                \"val_precision\": val_precision,\n",
    "                \"val_recall\": val_recall,\n",
    "                \"val_f1\": val_f1,\n",
    "            })\n",
    "\n",
    "            metrics_path = os.path.join(checkpoint_dir, f'training_metrics_epoch_{epoch + 1}.csv')\n",
    "            with open(metrics_path, 'w', newline='') as csvfile:\n",
    "                fieldnames = ['epoch', 'train_loss', 'train_accuracy', 'train_precision', 'train_recall', 'train_f1',\n",
    "                              'val_loss', 'val_accuracy', 'val_precision', 'val_recall', 'val_f1']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                for metric in metrics:\n",
    "                    writer.writerow(metric)\n",
    "\n",
    "            print(f\"Metriche per l'epoca {epoch + 1} salvate in: {metrics_path}\")\n",
    "\n",
    "            if epoch_val_loss < best_val_loss:\n",
    "                best_val_loss = epoch_val_loss\n",
    "                patience_counter = 0\n",
    "                performance_drop_counter = 0\n",
    "                checkpoint_state = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'metrics': metrics\n",
    "                }\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, f'best_checkpoint.pth')\n",
    "                save_checkpoint(checkpoint_state, checkpoint_path)\n",
    "                print(f\"Miglior checkpoint salvato in: {checkpoint_path}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if last_val_metrics:\n",
    "                if (val_accuracy < last_val_metrics['val_accuracy'] and\n",
    "                        val_precision < last_val_metrics['val_precision'] and\n",
    "                        val_recall < last_val_metrics['val_recall'] and\n",
    "                        val_f1 < last_val_metrics['val_f1']):\n",
    "                    performance_drop_counter += 1\n",
    "                    print(f\"Prestazioni calate all'epoca {epoch + 1}, contatore di calo: {performance_drop_counter}\")\n",
    "                else:\n",
    "                    performance_drop_counter = 0\n",
    "\n",
    "            last_val_metrics = {\n",
    "                \"val_accuracy\": val_accuracy,\n",
    "                \"val_precision\": val_precision,\n",
    "                \"val_recall\": val_recall,\n",
    "                \"val_f1\": val_f1,\n",
    "            }\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping attivato dopo {epoch + 1} epoche senza miglioramenti\")\n",
    "                break\n",
    "\n",
    "            if performance_drop_counter >= performance_drop_patience:\n",
    "                print(f\"Addestramento interrotto dopo {epoch + 1} epoche di calo delle prestazioni consecutive\")\n",
    "                break"
   ],
   "id": "dc7f5cfc373af694",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Salva Modello",
   "id": "10343d14ed28ef1c"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "    torch.save(model.state_dict(), 'EfficientNetB3_binary_classification.pth')\n",
    "    model_path = os.path.abspath('EfficientNetB3_binary_classification.pth')\n",
    "    print(f\"Modello salvato in: {model_path}\")"
   ],
   "id": "96594c85c89b5aaf",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Grafico Allenamento e Valutazione",
   "id": "78c2a91cbc85a634"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T08:59:35.435566Z",
     "start_time": "2024-06-08T08:59:34.774804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Leggi i dati dal file CSV\n",
    "df = pd.read_csv(r'/Esperimento 1/EfficientNetB3_Binary\\training_metrics_epoch_7.csv')\n",
    "\n",
    "# Estrai le metriche di addestramento e validazione\n",
    "train_metrics = ['train_loss', 'train_accuracy', 'train_precision', 'train_recall', 'train_f1']\n",
    "val_metrics = ['val_loss', 'val_accuracy', 'val_precision', 'val_recall', 'val_f1']\n",
    "\n",
    "# Crea un grafico separato per ogni coppia di metriche\n",
    "for train_metric, val_metric in zip(train_metrics, val_metrics):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df['epoch'], df[train_metric], label='Train ' + train_metric.split('_')[1].capitalize())\n",
    "    plt.plot(df['epoch'], df[val_metric], label='Validation ' + val_metric.split('_')[1].capitalize())\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.title(train_metric.split('_')[1].capitalize() + ' Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "id": "18deafd18d6f6b68",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Testing",
   "id": "d01b537145d8a24b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T09:23:23.286085Z",
     "start_time": "2024-06-08T09:22:42.547273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_model(model_path, device):\n",
    "   # Caricamento del modello EfficientNet B3 pre-addestrato\n",
    "    model = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.DEFAULT)\n",
    "\n",
    "    # Modifica dell'ultimo strato per la classificazione binaria\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, 1)\n",
    "   \n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def test_model(model_path, test_dataset_path, checkpoint_dir):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    test_dataset = CustomDataset(test_dataset_path, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    model = load_model(model_path, device)\n",
    "    model.eval()\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    running_test_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            test_loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "            running_test_loss += test_loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_outputs.extend(outputs.cpu().numpy())\n",
    "\n",
    "    test_loss = running_test_loss / len(test_loader)\n",
    "    test_accuracy, test_precision, test_recall, test_f1 = calculate_metrics(test_loader, model, device)\n",
    "\n",
    "    # Creazione del file CSV nella directory dei checkpoint\n",
    "    csv_output_path = os.path.join(checkpoint_dir, 'test_results.csv')\n",
    "    with open(csv_output_path, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['test_loss', 'test_accuracy', 'test_precision', 'test_recall', 'test_f1']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerow({\n",
    "            'test_loss': test_loss,\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'test_precision': test_precision,\n",
    "            'test_recall': test_recall,\n",
    "            'test_f1': test_f1\n",
    "        })\n",
    "\n",
    "    print(f\"Risultati del testing salvati in: {csv_output_path}\")\n",
    "\n",
    "# Esempio di utilizzo\n",
    "model_path = r'C:\\Users\\biagi\\PycharmProjects\\gruppo17\\Modello\\EfficientNetB3_binary_classification.pth'\n",
    "test_dataset_path = r'C:\\Users\\biagi\\PycharmProjects\\gruppo17\\Testing_Norm'\n",
    "checkpoint_dir = r'/Esperimento 1/EfficientNetB3_Binary'\n",
    "\n",
    "test_model(model_path, test_dataset_path, checkpoint_dir)"
   ],
   "id": "62b66bde9f055547",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Grafico Testing\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "def plot_results(csv_file):\n",
    "    results = {}\n",
    "    with open(csv_file, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            for key, value in row.items():\n",
    "                results[key] = float(value)\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Bar plot\n",
    "    ax.bar(results.keys(), results.values(), color='skyblue')\n",
    "\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title('Testing Metrics')\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Esempio di utilizzo\n",
    "csv_file = r'/Modello/EfficientNetB3_Binary/test_results.csv'\n",
    "plot_results(csv_file)"
   ],
   "id": "e6ad08acd4a20ad8",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
