{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Descrizione",
   "id": "922ce7a0a6553761"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Nell'esperimento è stato utilizzato il modello AlexNet per la classificazione binaria delle sottoclassi Target. Non viene usato alcun meccanismo per bloccare autonomamente il treaning del modello, eseguendo tutte e 50 le epoche. Il PreProcessing è invariato.",
   "id": "ec229eaccee0a7c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Librerie",
   "id": "54a1c84017e80d8d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T21:07:23.924822Z",
     "start_time": "2024-06-12T21:07:22.464297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import os\n",
    "import csv\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "c97984dbb9994664",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Inizializzazione dei parametri",
   "id": "539794cc50743bc2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T15:16:02.753569Z",
     "start_time": "2024-06-09T15:16:02.751564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Percorsi dei dataset di addestramento, validazione e test\n",
    "train_dataset_path = '/Users/massimo/PycharmProjects/UnderwaterSoundsClassification/Classificazione_Multiclasse/Training_Balanced_Target'\n",
    "\n",
    "val_dataset_path = '/Users/massimo/PycharmProjects/UnderwaterSoundsClassification/Classificazione_Multiclasse/Evaluation_Balanced_Target'\n",
    "\n",
    "test_dataset_path = '/Users/massimo/PycharmProjects/UnderwaterSoundsClassification/Classificazione_Multiclasse/Testing_Balanced_Target'\n",
    "\n",
    "# Percorso della directory per salvare i checkpoint\n",
    "checkpoint_dir = '/Users/massimo/PycharmProjects/UnderwaterSoundsClassification/Esperimento 1 Multiclasse/AlexNet'\n",
    "\n",
    "# Numero di epoche di addestramento\n",
    "num_epochs = 50\n",
    "\n",
    "# Dimensione del batch per l'addestramento e la validazione\n",
    "batch_size = 128\n",
    "\n",
    "# Tasso di apprendimento per l'ottimizzatore\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Numero di passaggi per accumulare gradienti prima di aggiornare i pesi\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "# Definizione della trasformazione per il preprocessing delle immagini\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Nome del file CSV in cui salvare le metriche di addestramento\n",
    "metrics_path = os.path.join(checkpoint_dir, 'training_metrics.csv')"
   ],
   "id": "ad0a649f07e17835",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Definizione della Classe CustomDataset",
   "id": "ee87018fc940a175"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T15:16:02.756612Z",
     "start_time": "2024-06-09T15:16:02.754015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted([d for d in os.listdir(self.root_dir) if os.path.isdir(os.path.join(self.root_dir, d))])\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "\n",
    "        for idx, cls in enumerate(self.classes):\n",
    "            class_dir = os.path.join(self.root_dir, cls)\n",
    "            for file in os.listdir(class_dir):\n",
    "                file_path = os.path.join(class_dir, file)\n",
    "                if os.path.isfile(file_path):\n",
    "                    self.data.append(file_path)\n",
    "                    self.targets.append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target"
   ],
   "id": "a6bcddf63c1f468e",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Funzioni per il Caricamento e il Salvataggio dei Checkpoint",
   "id": "483d7bfa1b385f0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T15:16:02.758854Z",
     "start_time": "2024-06-09T15:16:02.757210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    start_epoch += 1\n",
    "    metrics = checkpoint['metrics']\n",
    "    return model, optimizer, start_epoch, metrics\n",
    "\n",
    "def save_checkpoint(checkpoint_state, checkpoint_path):\n",
    "    torch.save(checkpoint_state, checkpoint_path)"
   ],
   "id": "41caa1a00c379995",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Funzione per il Calcolo delle Metriche",
   "id": "d2c0cf74f83e1132"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T15:16:02.761976Z",
     "start_time": "2024-06-09T15:16:02.759914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_metrics(loader, model, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(loader, desc=\"Calcolo metriche\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds)\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ],
   "id": "b0fd067a918634db",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Preparazione dei Dati",
   "id": "600d338e067b821b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T15:16:03.040701Z",
     "start_time": "2024-06-09T15:16:02.762423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Dataset import CustomDataset\n",
    "\n",
    "train_dataset = CustomDataset(train_dataset_path, transform=transform)\n",
    "val_dataset = CustomDataset(val_dataset_path, transform=transform)\n",
    "\n",
    "print(f\"Classi trovate nel dataset di addestramento: {train_dataset.classes}\")\n",
    "print(f\"Classi trovate nel dataset di validazione: {val_dataset.classes}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)"
   ],
   "id": "d25643cf401b041a",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Inizializzazione del Modello",
   "id": "d7fbd9c47782a669"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T15:16:03.226420Z",
     "start_time": "2024-06-09T15:16:03.041268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_classes = len(train_dataset.classes)\n",
    "model = models.alexnet(weights=models.AlexNet_Weights.DEFAULT)\n",
    "model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
    "\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "scaler = GradScaler()\n"
   ],
   "id": "e46858fe36d59972",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Caricamento del Checkpoint",
   "id": "896ceef2bae0c16b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T15:16:03.387085Z",
     "start_time": "2024-06-09T15:16:03.226995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "latest_checkpoint = glob.glob(os.path.join(checkpoint_dir, 'checkpoint_epoch_*.pth'))\n",
    "if latest_checkpoint:\n",
    "    latest_checkpoint = max(latest_checkpoint, key=os.path.getctime)\n",
    "    model, optimizer, start_epoch, metrics = load_checkpoint(latest_checkpoint, model, optimizer)\n",
    "else:\n",
    "    model, optimizer, start_epoch, metrics = model, optimizer, 0, []"
   ],
   "id": "4108389bf61ee458",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ciclo di Addestramento e Valutazione",
   "id": "b9babe10e812cb1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T15:16:05.760013Z",
     "start_time": "2024-06-09T15:16:05.752439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "header_written = False\n",
    "if start_epoch >= num_epochs:\n",
    "        print(f\"Modello già addestrato fino all'epoca {num_epochs}.\")\n",
    "else:\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        pbar_train = tqdm(train_loader, desc=f\"Epoca {epoch + 1}/{num_epochs} - Addestramento\")\n",
    "        for step, (inputs, labels) in enumerate(pbar_train):\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                train_loss = criterion(outputs, labels)\n",
    "            scaler.scale(train_loss).backward()\n",
    "\n",
    "            # Accumula gradienti\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            running_train_loss += train_loss.item()\n",
    "            pbar_train.set_postfix({'Loss': running_train_loss / (step + 1)})\n",
    "\n",
    "        # Valutazione del modello ad ogni epoca\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        pbar_val = tqdm(val_loader, desc=f\"Epoca {epoch + 1}/{num_epochs} - Valutazione\")\n",
    "        for inputs, labels in pbar_val:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            with torch.no_grad():\n",
    "                with autocast():\n",
    "                    outputs = model(inputs)\n",
    "                    val_loss = criterion(outputs, labels)\n",
    "                running_val_loss += val_loss.item()\n",
    "                pbar_val.set_postfix({'Val Loss': running_val_loss / len(val_loader)})\n",
    "\n",
    "        # Calcola le metriche di addestramento e validazione\n",
    "        train_accuracy, train_precision, train_recall, train_f1 = calculate_metrics(train_loader, model, device)\n",
    "        val_accuracy, val_precision, val_recall, val_f1 = calculate_metrics(val_loader, model, device)\n",
    "        epoch_train_loss = running_train_loss / len(train_loader)\n",
    "        epoch_val_loss = running_val_loss / len(val_loader)\n",
    "\n",
    "        # Stampa le metriche\n",
    "        print(f\"Epoca {epoch + 1}/{num_epochs} - \"\n",
    "              f\"Loss Addestramento: {epoch_train_loss:.4f} - Acc Addestramento: {train_accuracy:.4f} - \"\n",
    "              f\"Precision Addestramento: {train_precision:.4f} - Recall Addestramento: {train_recall:.4f} - \"\n",
    "              f\"F1 Addestramento: {train_f1:.4f} - \"\n",
    "              f\"Loss Valutazione: {epoch_val_loss:.4f} - Acc Valutazione: {val_accuracy:.4f} - \"\n",
    "              f\"Precision Valutazione: {val_precision:.4f} - Recall Valutazione: {val_recall:.4f} - F1 Valutazione: {val_f1:.4f}\")\n",
    "\n",
    "        # Aggiorna il file CSV con le metriche\n",
    "        with open(metrics_path, 'a', newline='') as csvfile:\n",
    "            fieldnames = ['epoch', 'train_loss', 'train_accuracy', 'train_precision', 'train_recall', 'train_f1',\n",
    "                          'val_loss', 'val_accuracy', 'val_precision', 'val_recall', 'val_f1']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            # Scrittura dell'header solo se non è stato già scritto\n",
    "            if not header_written:\n",
    "                writer.writeheader()\n",
    "                header_written = True\n",
    "\n",
    "            # Scrittura delle metriche per l'epoca corrente\n",
    "            writer.writerow({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": epoch_train_loss,\n",
    "                \"train_accuracy\": train_accuracy,\n",
    "                \"train_precision\": train_precision,\n",
    "                \"train_recall\": train_recall,\n",
    "                \"train_f1\": train_f1,\n",
    "                \"val_loss\": epoch_val_loss,\n",
    "                \"val_accuracy\": val_accuracy,\n",
    "                \"val_precision\": val_precision,\n",
    "                \"val_recall\": val_recall,\n",
    "                \"val_f1\": val_f1,\n",
    "            })\n",
    "\n",
    "        # Salva il checkpoint\n",
    "        checkpoint_state = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch + 1}.pth')\n",
    "        save_checkpoint(checkpoint_state, checkpoint_path)\n",
    "        print(f\"Checkpoint salvato in: {checkpoint_path}\")"
   ],
   "id": "7625c2c670a801dc",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Salvataggio del Modello Finale",
   "id": "4e4edacf9bca4cfb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_path = '/Users/massimo/PycharmProjects/UnderwaterSoundsClassification/Esperimento 1 Multiclasse/AlexNet/alexnet_model_multiclass_classification.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(f\"Modello finale salvato in: {model_path}\")"
   ],
   "id": "75746430e9078031",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Creazione dei grafici di Addestramento e Validazione",
   "id": "3a81a48022d353a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T15:16:47.947422Z",
     "start_time": "2024-06-09T15:16:47.664613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(os.path.join(checkpoint_dir, 'training_metrics.csv'))\n",
    "\n",
    "# Estrai le metriche di addestramento e validazione\n",
    "train_metrics = ['train_loss', 'train_accuracy', 'train_precision', 'train_recall', 'train_f1']\n",
    "val_metrics = ['val_loss', 'val_accuracy', 'val_precision', 'val_recall', 'val_f1']\n",
    "\n",
    "# Crea un grafico separato per ogni coppia di metriche\n",
    "for train_metric, val_metric in zip(train_metrics, val_metrics):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df['epoch'], df[train_metric], label='Train ' + train_metric.split('_')[1].capitalize())\n",
    "    plt.plot(df['epoch'], df[val_metric], label='Validation ' + val_metric.split('_')[1].capitalize())\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.title(train_metric.split('_')[1].capitalize() + ' Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "id": "b49db918f789d3e7",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Testing",
   "id": "36f8ad89a0eaad8d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T21:07:29.077178Z",
     "start_time": "2024-06-12T21:07:28.923046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_model(model_path, num_classes, device):\n",
    "    model = models.alexnet(weights=models.AlexNet_Weights.DEFAULT)\n",
    "    model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_dataset = CustomDataset(test_dataset_path, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "num_classes = len(test_dataset.classes)\n",
    "model = load_model(model_path, num_classes, device)\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "running_test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        outputs = model(inputs)\n",
    "        test_loss = criterion(outputs, labels)\n",
    "        running_test_loss += test_loss.item()\n",
    "\n",
    "test_loss = running_test_loss / len(test_loader)\n",
    "test_accuracy, test_precision, test_recall, test_f1 = calculate_metrics(test_loader, model, device)\n",
    "\n",
    "# Creazione del file CSV nella directory dei checkpoint\n",
    "csv_output_path = os.path.join(checkpoint_dir, 'test_results.csv')\n",
    "with open(csv_output_path, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['test_loss', 'test_accuracy', 'test_precision', 'test_recall', 'test_f1']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerow({\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_precision': test_precision,\n",
    "        'test_recall': test_recall,\n",
    "        'test_f1': test_f1\n",
    "    })\n",
    "\n",
    "print(f\"Risultati del testing salvati in: {csv_output_path}\")\n"
   ],
   "id": "4390cea64b0a0f21",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Creazione Grafici Testing",
   "id": "4ac3182c6838dc9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T15:20:25.171038Z",
     "start_time": "2024-06-09T15:20:25.083521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_results(csv_file):\n",
    "    results = {}\n",
    "    with open(csv_file, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            for key, value in row.items():\n",
    "                results[key] = float(value)\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Bar plot\n",
    "    ax.bar(results.keys(), results.values(), color='skyblue')\n",
    "\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title('Testing Metrics')\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Esempio di utilizzo\n",
    "csv_file = os.path.join(checkpoint_dir, 'test_results.csv')\n",
    "plot_results(csv_file)\n"
   ],
   "id": "9a85a757c28e484e",
   "execution_count": 12,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
